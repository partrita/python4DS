{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95f0a171",
   "metadata": {},
   "source": [
    "(webscraping-and-apis)=\n",
    "# 웹 스크래핑 및 API\n",
    "\n",
    "## 소개\n",
    "\n",
    "이 장에서는 웹 스크래핑을 통해 웹 페이지에서 가져오거나 API를 통해 인터넷을 통해 직접 가져오는 온라인 데이터로 작업하는 방법을 보여줍니다. 중요한 원칙은 API가 있는 경우 항상 API를 사용하는 것입니다. 이는 정보를 파이썬 세션으로 직접 전달하도록 설계되었으며 많은 노력을 절약할 수 있기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a55374",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# 셀 제거\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "\n",
    "# 플롯 설정\n",
    "plt.style.use(\"https://github.com/aeturrell/python4DS/raw/main/plot_style.txt\")\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats(\"svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b3d11c",
   "metadata": {},
   "source": [
    "### 윤리적 및 법적 고려 사항\n",
    "\n",
    "이미 말했듯이 가능하면 항상 API를 사용해야 합니다. 하지만 불가능한 경우 웹 스크래핑과 관련하여 게임의 규칙은 무엇일까요? 먼저, 그렇게 하는 것이 합법적이고 윤리적인지 여부에 대해 이야기해야 합니다. 전반적으로 상황은 이 두 가지 모두와 관련하여 복잡합니다.\n",
    "\n",
    "합법성은 거주 지역에 따라 크게 달라집니다. 그러나 일반적인 원칙으로 데이터가 공개적이고 비개인적이며 비상업적이고 사실인 경우 괜찮을 가능성이 높습니다. 이러한 세 가지 요소는 아래에서 설명할 사이트의 이용 약관, 개인 식별 정보 및 저작권과 관련되어 있기 때문에 중요합니다.\n",
    "\n",
    "데이터가 공개적이지 않거나 비개인적이거나 사실이 아니거나 돈을 벌기 위해 특별히 데이터를 스크래핑하거나 상업적으로만 사용할 수 있는 것을 스크래핑하는 경우 변호사와 상담해야 합니다. 어떤 경우든 스크래핑하는 페이지를 호스팅하는 서버의 리소스를 존중해야 합니다. 가장 중요한 것은 여러 페이지를 스크래핑하는 경우 각 요청 사이에 약간의 시간을 기다려야 한다는 것입니다. [Tenacity](https://tenacity.readthedocs.io/en/latest/)는 이 작업에 훌륭한 패키지입니다. 이 패키지에는 무언가를 시도하는 사이에 일시 중지하는 기능(뿐만 아니라 다른 유용한 기능도 많이 있음)이 있습니다.\n",
    "\n",
    "자세히 살펴보면 많은 웹사이트 페이지 어딘가에 \"이용 약관\" 또는 \"서비스 약관\" 링크가 포함되어 있으며 해당 페이지를 자세히 읽어보면 사이트에서 웹 스크래핑을 명시적으로 금지하는 경우가 많습니다. 이러한 페이지는 회사가 매우 광범위한 주장을 하는 법적 토지 점유 경향이 있습니다. 가능한 경우 이러한 서비스 약관을 존중하는 것이 예의이지만 모든 주장을 액면 그대로 받아들이지는 마십시오.\n",
    "\n",
    "미국 법원은 일반적으로 웹사이트 바닥글에 서비스 약관을 단순히 게시하는 것만으로는 해당 약관에 구속되기에 충분하지 않다고 판결했습니다. 예를 들어 [HiQ Labs 대 LinkedIn](https://en.wikipedia.org/wiki/HiQ_Labs_v._LinkedIn) 사건이 있습니다. 일반적으로 서비스 약관에 구속되려면 계정을 만들거나 확인란을 선택하는 것과 같은 명시적인 조치를 취해야 합니다. 이것이 데이터가 **공개**인지 여부가 중요한 이유입니다. 데이터에 액세스하는 데 계정이 필요하지 않은 경우 서비스 약관에 구속될 가능성이 거의 없습니다. 그러나 유럽에서는 명시적으로 동의하지 않더라도 서비스 약관이 시행 가능하다고 법원이 판결한 상황이 다소 다릅니다.\n",
    "\n",
    "데이터가 공개적이더라도 이름, 이메일 주소, 전화번호, 생년월일 등과 같은 개인 식별 정보를 스크래핑하는 데는 극도로 주의해야 합니다. 유럽에는 이러한 데이터 수집 또는 저장에 대한 엄격한 법률(GDPR이라고 함)이 있으며 거주 지역에 관계없이 윤리적 수렁에 빠질 가능성이 높습니다. 예를 들어 2016년에 한 연구 그룹은 데이트 사이트 OkCupid에서 70,000명의 공개 프로필 정보(예: 사용자 이름, 나이, 성별, 위치 등)를 스크래핑하여 익명화 시도 없이 이러한 데이터를 공개적으로 발표했습니다. 연구자들은 데이터가 이미 공개되어 있었기 때문에 문제가 없다고 생각했지만 이 작업은 데이터 세트에 정보가 공개된 사용자의 식별 가능성에 대한 윤리적 우려로 인해 널리 비난을 받았습니다. 작업에 개인 식별 정보 스크래핑이 포함되는 경우 OkCupid 연구3과 개인 식별 정보 획득 및 공개와 관련된 연구 윤리가 의심스러운 유사한 연구에 대해 읽어보는 것이 좋습니다.\n",
    "\n",
    "마지막으로 저작권법에 대해서도 걱정해야 합니다. 저작권법은 복잡합니다. 미국법은 보호되는 대상을 정확하게 설명합니다. \"[…] 유형의 표현 매체에 고정된 독창적인 저작물, […]\". 그런 다음 문학 작품, 음악 작품, 영화 등과 같이 적용되는 특정 범주를 설명합니다. 특히 저작권 보호에서 제외되는 것은 데이터입니다. 즉, 스크래핑을 사실로 제한하는 한 저작권 보호는 적용되지 않습니다. (그러나 유럽에는 데이터베이스를 보호하는 별도의 \"특별\" 권리가 있다는 점에 유의하십시오.)\n",
    "\n",
    "간단한 예를 들면 미국에서는 재료 및 지침 목록은 저작권 보호 대상이 아니므로 저작권을 사용하여 레시피를 보호할 수 없습니다. 그러나 해당 레시피 목록에 상당한 새로운 문학적 내용이 수반되는 경우 이는 저작권 보호 대상입니다. 이것이 인터넷에서 레시피를 찾을 때 항상 내용이 많은 이유입니다.\n",
    "\n",
    "텍스트나 이미지와 같은 원본 콘텐츠를 스크래핑해야 하는 경우에도 공정 사용 원칙에 따라 보호받을 수 있습니다. 공정 사용은 명확한 규칙이 아니라 여러 요소를 고려합니다. 연구 또는 비상업적 목적으로 데이터를 수집하고 필요한 것만 스크래핑하도록 제한하는 경우 적용될 가능성이 더 높습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17575f3a",
   "metadata": {},
   "source": [
    "### 전제 조건\n",
    "\n",
    "이 장에서는 **pandas** 패키지를 설치해야 합니다. 이미 설치되어 있어야 하는 **seaborn**도 사용할 것입니다. 또한 터미널에서 각각 `pip install beautifulsoup4` 및 `pip install pandas-datareader`를 실행하여 **beautifulsoup** 및 **pandas-datareader** 패키지를 설치해야 합니다. 또한 두 가지 내장 패키지인 **textwrap** 및 **requests**도 사용할 것입니다.\n",
    "\n",
    "시작하려면 필요한 패키지 중 일부를 가져옵니다(스크립트나 노트북 상단에 필요한 패키지를 가져오는 것은 항상 좋은 습관입니다)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f62293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from lets_plot import *\n",
    "\n",
    "LetsPlot.setup_html()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f43a5237",
   "metadata": {},
   "source": [
    "## **pandas**를 사용하여 인터넷에서 파일 데이터 추출하기\n",
    "\n",
    "URL과 파일 형식을 알고 나면 인터넷에서 데이터를 쉽게 읽을 수 있습니다. 예를 들어 다음은 URL에 CSV 파일로 저장된 'storms' 데이터 세트를 읽어오는 예입니다(처음 10개 행만 가져옴)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06108a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\n",
    "    \"https://vincentarelbundock.github.io/Rdatasets/csv/dplyr/storms.csv\", nrows=10\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4223e854",
   "metadata": {},
   "source": [
    "## API를 사용하여 데이터 가져오기\n",
    "\n",
    "API(애플리케이션 프로그래밍 인터페이스)를 사용하는 것은 인터넷에서 정보를 가져오는 또 다른 방법입니다. API는 파이썬과 같은 한 도구가 서버와 같은 다른 도구와 통신하고 유용하게 정보를 교환하는 방법일 뿐입니다. 일반적인 사용 사례는 API를 통해 특정 쿼리에 맞는 데이터 요청을 게시하고 해당 데이터 다운로드를 받는 것입니다. (사이트를 웹 스크래핑하는 것보다 항상 API를 우선적으로 사용해야 합니다.)\n",
    "\n",
    "모든 도구와 함께 작동하도록 설계되었기 때문에 실제로 API와 상호 작용하는 데 프로그래밍 언어가 필요하지 않지만 프로그래밍 언어를 사용하면 훨씬 쉽습니다.\n",
    "\n",
    "```{note}\n",
    "일부 API에 액세스하려면 API 키가 필요합니다. 때로는 사이트에 등록하기만 하면 되지만 다른 경우에는 액세스 비용을 지불해야 할 수도 있습니다.\n",
    "```\n",
    "\n",
    "이를 확인하기 위해 API를 직접 사용하여 일부 시계열 데이터를 가져오겠습니다. **requests** 패키지를 사용하여 인터넷으로 호출합니다.\n",
    "\n",
    "API에는 '엔드포인트'(기본 URL)와 질문을 인코딩하는 URL이 있습니다. 엔드포인트가 \"https://api.beta.ons.gov.uk/v1/\"인 ONS API를 예로 들어 보겠습니다. API의 나머지 부분은 'data?uri=' 형식이며 그 뒤에 시계열(jp9z)과 데이터 세트(LMS)의 긴 ID가 옵니다. 이는 영국 서비스 부문의 공석입니다.\n",
    "\n",
    "API에서 반환되는 데이터는 일반적으로 JSON 형식이며 중첩된 파이썬 사전과 매우 유사하며 동일한 방식으로 항목에 액세스할 수 있습니다. 아래 예에서 시리즈 제목을 가져올 때 발생하는 상황입니다. JSON은 분석에 적합하지 않으므로 **pandas**를 사용하여 데이터를 정리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4226d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.beta.ons.gov.uk/v1/data?uri=/employmentandlabourmarket/peopleinwork/employmentandemployeetypes/timeseries/jp9z/lms/previous/v108\"\n",
    "\n",
    "# ONS API에서 데이터 가져오기:\n",
    "json_data = requests.get(url).json()\n",
    "\n",
    "# 빠른 플롯을 위해 데이터 준비\n",
    "title = json_data[\"description\"][\"title\"]\n",
    "df = (\n",
    "    pd.DataFrame(pd.json_normalize(json_data[\"months\"]))\n",
    "    .assign(\n",
    "        date=lambda x: pd.to_datetime(x[\"date\"]),\n",
    "        value=lambda x: pd.to_numeric(x[\"value\"]),\n",
    "    )\n",
    "    .set_index(\"date\")\n",
    ")\n",
    "\n",
    "df[\"value\"].plot(title=title, ylim=(0, df[\"value\"].max() * 1.2), lw=3.0);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "670ce0bb",
   "metadata": {},
   "source": [
    "API *읽기*에 대해 이야기했습니다. 데이터, 모델 등 원하는 모든 것을 제공하기 위해 자신만의 API를 만들 수도 있습니다! 이것은 고급 주제이며 여기서 다루지 않겠지만 필요한 경우 가장 간단한 방법은 [Fast API](https://fastapi.tiangolo.com/)를 사용하는 것입니다. Fast API에 대한 짧은 비디오 튜토리얼은 [여기](https://calmcode.io/fastapi/hello-world.html)에서 찾을 수 있습니다.\n",
    "\n",
    "### Pandas Datareader: (일부) API와 더 쉽게 상호 작용하는 방법\n",
    "\n",
    "ONS 데이터를 가져오는 데 많은 코드가 필요하지 않았지만 한 줄이면 훨씬 더 좋을 것입니다. 그렇지 않습니까? 다행히 이를 쉽게 만들어주는 몇 가지 패키지가 있지만 API에 따라 다르며(API는 시간이 지남에 따라 변경됨) API에 따라 다릅니다.\n",
    "\n",
    "추가 API에 액세스하기 위한 가장 포괄적인 라이브러리는 [**pandas-datareader**](https://pandas-datareader.readthedocs.io/en/latest/)이며 다음 항목에 편리하게 액세스할 수 있습니다.\n",
    "\n",
    "- FRED\n",
    "- Quandl\n",
    "- 세계은행\n",
    "- OECD\n",
    "- 유로스타트\n",
    "\n",
    "등이 있습니다.\n",
    "\n",
    "FRED(세인트루이스 연방준비은행 경제 데이터 라이브러리)를 사용한 예를 살펴보겠습니다. 이번에는 영국 실업률을 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf758fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_datareader.data as web\n",
    "\n",
    "df_u = web.DataReader(\"LRHUTTTTGBM156S\", \"fred\")\n",
    "\n",
    "df_u.plot(title=\"영국 실업률 (%)\", legend=False, ylim=(2, 6), lw=3.0);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0613aefb",
   "metadata": {},
   "source": [
    "그리고 매우 유용하기 때문에 **pandas-datareader**를 사용하여 세계은행 데이터에 액세스하는 방법도 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380ca743",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# 세계은행 총 온실가스 배출량 (1인당 CO2e 미터톤)\n",
    "# https://data.worldbank.org/indicator/EN.GHG.ALL.PC.CE.AR5\n",
    "# 국가 및 지역 코드는 http://api.worldbank.org/v2/country\n",
    "from pandas_datareader import wb\n",
    "\n",
    "df = wb.download(  # 세계은행에서 데이터 다운로드\n",
    "    indicator=\"EN.GHG.ALL.PC.CE.AR5\",  # 지표 코드\n",
    "    country=[\"US\", \"CHN\", \"IND\", \"Z4\", \"Z7\"],  # 국가 코드\n",
    "    start=2019,  # 시작 연도\n",
    "    end=2019,  # 종료 연도\n",
    ")\n",
    "df = df.reset_index()  # 인덱스로서의 국가 제거\n",
    "df[\"country\"] = df[\"country\"].apply(lambda x: textwrap.fill(x, 10))  # 긴 이름 줄 바꿈\n",
    "df = df.sort_values(\"EN.GHG.ALL.PC.CE.AR5\")  # 재정렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00379b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    ggplot(df, aes(x=\"country\", y=\"EN.ATM.CO2E.PC\"))\n",
    "    + geom_bar(aes(fill=\"country\"), color=\"black\", alpha=0.8, stat=\"identity\")\n",
    "    + scale_fill_discrete()\n",
    "    + theme_minimal()\n",
    "    + theme(legend_position=\"none\")\n",
    "    + ggsize(600, 400)\n",
    "    + labs(\n",
    "        subtitle=\"이산화탄소 (1인당 미터톤)\",\n",
    "        title=\"미국이 1인당 배출량에서 세계를 선도합니다\",\n",
    "        y=\"\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7bf16d7",
   "metadata": {},
   "source": [
    "### OECD API\n",
    "\n",
    "때로는 API를 직접 사용하는 것이 편리하며, 예를 들어 OECD API는 직접 액세스를 통해 활용할 수 있는 복잡성이 매우 높습니다. OECD API는 JSON 및 XML 형식으로 데이터를 제공하며, XML 형식 데이터를 가져와 일반적인 **pandas** 데이터 프레임으로 변환하기 위해 [**pandasdmx**](https://pandasdmx.readthedocs.io/)(Python 데이터 생태계를 위한 통계 데이터 및 메타데이터 교환(SDMX) 패키지라고도 함)를 사용할 것입니다.\n",
    "\n",
    "이제 OECD API를 사용하는 핵심은 국가, 시간, 리소스 및 시리즈에 대한 많은 코드를 아는 것입니다. API가 사용하는 코드에 대한 일반적인 지침은 [여기](https://data.oecd.org/api/sdmx-ml-documentation/)에서 찾을 수 있지만 필요한 것을 정확히 찾는 것은 약간 까다로울 수 있습니다. 두 가지 팁은 다음과 같습니다.\n",
    "1. 찾고 있는 것이 특정 명명된 데이터 세트(예: \"QNA\"(분기별 국가 계정))에 있다는 것을 알고 있다면 브라우저에 `https://stats.oecd.org/restsdmx/sdmx.ashx/GetDataStructure/QNA/all?format=SDMX-ML`을 입력하고 XML 파일을 살펴보십시오. 하위 코드와 사용 가능한 국가를 선택할 수 있습니다.\n",
    "2. https://stats.oecd.org/에서 탐색하고 사용자 정의를 사용한 다음 모든 \"코드 사용\" 확인란을 선택하여 탐색 중인 코드 이름을 확인합니다.\n",
    "\n",
    "실제로 작동하는 예를 살펴보겠습니다. 2010년 이후 여러 국가의 생산성(시간당 GDP) 데이터를 보고 싶습니다. 생산성 리소스(코드 \"PDB_LV\")에 있으며 GDP의 USD 현재 가격(코드 \"CPC\") 측정값인 고용된 근로자당 GDP(코드 \"T_GDPEMP)를 2010년 이후(코드 \"startTime=2010\")부터 가져올 것입니다. 생산성 측정이 약간 더 비교 가능할 수 있는 일부 선진국에 대해 이를 가져올 것입니다. 아래 주석은 각 단계에서 무슨 일이 일어나고 있는지 설명합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "521d4d03",
   "metadata": {},
   "source": [
    "```python\n",
    "import pandasdmx as pdmx\n",
    "# pdmx에 OECD 데이터를 원한다고 알립니다.\n",
    "oecd = pdmx.Request(\"OECD\")\n",
    "# OECD API에서 지정한 형식으로 요청에 대한 모든 것을 설정합니다.\n",
    "data = oecd.data(\n",
    "    resource_id=\"PDB_LV\",\n",
    "    key=\"GBR+FRA+CAN+ITA+DEU+JPN+USA.T_GDPEMP.CPC/all?startTime=2010\",\n",
    ").to_pandas()\n",
    "\n",
    "df = pd.DataFrame(data).reset_index()\n",
    "df.head()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5cac233",
   "metadata": {},
   "source": [
    "|   | LOCATION |  SUBJECT | MEASURE | TIME_PERIOD |        value |\n",
    "|--:|---------:|---------:|--------:|------------:|-------------:|\n",
    "| 0 |      CAN | T_GDPEMP |     CPC |        2010 | 78848.604088 |\n",
    "| 1 |      CAN | T_GDPEMP |     CPC |        2011 | 81422.364748 |\n",
    "| 2 |      CAN | T_GDPEMP |     CPC |        2012 | 82663.028058 |\n",
    "| 3 |      CAN | T_GDPEMP |     CPC |        2013 | 86368.582158 |\n",
    "| 4 |      CAN | T_GDPEMP |     CPC |        2014 | 89617.632446 |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "302326b4",
   "metadata": {},
   "source": [
    "잘 됐습니다! 깔끔한 형식의 데이터를 얻었습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7849ac74",
   "metadata": {},
   "source": [
    "### 기타 유용한 API\n",
    "\n",
    "- github의 [이 공개 API 저장소](https://github.com/public-apis/public-apis)에 정기적으로 업데이트되는 API 목록이 있습니다. 아직 경제학 섹션은 없지만 다른 API가 많이 있습니다.\n",
    "- 버클리 도서관은 살펴볼 가치가 있는 [경제학 API 목록](https://guides.lib.berkeley.edu/c.php?g=4395&p=7995952)을 유지 관리합니다.\n",
    "- [NASDAQ 데이터 링크](https://docs.data.nasdaq.com/)에는 많은 [금융 데이터](https://docs.data.nasdaq.com/docs/data-organization)가 있습니다.\n",
    "- [DBnomics](https://db.nomics.world/): 국가 및 국제 통계 기관뿐만 아니라 연구원 및 민간 기업에서 제공하는 공개적으로 사용 가능한 경제 데이터입니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd85df34",
   "metadata": {},
   "source": [
    "## 웹 스크래핑\n",
    "\n",
    "웹 스크래핑은 브라우저에 표시되도록 만들어진 인터넷에서 정보를 가져오는 방법입니다. 하지만 최후의 수단으로만 사용해야 하며 웹사이트의 이용 약관에서 허용하는 경우에만 사용해야 합니다.\n",
    "\n",
    "인터넷에서 데이터를 가져오는 경우 API가 있는 경우 항상 API를 사용하는 것이 훨씬 좋습니다. 구조화된 방식으로 정보를 가져오는 것이 바로 API가 존재하는 이유입니다. API는 또한 자주 변경될 수 있는 웹사이트보다 안정적이어야 합니다. 일반적으로 조직에서 데이터를 가져가는 것을 허용하는 경우 해당 목적을 위해 명시적으로 API를 만들었을 것입니다. 웹 스크래핑을 허용하지만 API가 없는 주요 웹사이트는 거의 없습니다. 이러한 웹사이트의 경우 API가 없으면 스크래핑이 이용 약관에 위배될 가능성이 높습니다. 이러한 이용 약관은 법으로 시행될 수 있습니다(여기서는 국가마다 규칙이 다르며 스크래핑 가능 여부가 명확하지 않은 경우 법률 자문이 정말 필요합니다).\n",
    "\n",
    "웹 스크래핑이 그다지 좋지 않은 다른 이유도 있습니다. 예를 들어 백런이 필요한 경우 API를 통해 제공될 수 있지만 웹 페이지에는 표시되지 않을 수 있습니다. (또는 전혀 사용할 수 없을 수도 있으며 이 경우 조직에 문의하거나 스냅샷을 찍었을 수 있으므로 WaybackMachine을 확인하는 것이 가장 좋습니다.)\n",
    "\n",
    "따라서 이 책은 거의 항상 더 나은 해결책이 있으므로 웹 스크래핑에 대해 상당히 부정적입니다. 그러나 유용한 경우도 있습니다.\n",
    "\n",
    "스크래핑 상황에 처한 경우 합법적으로 허용되는지, 웹사이트의 `robots.txt` 규칙을 위반하지 않는지 반드시 확인하십시오. 이것은 거의 모든 웹사이트에 있는 특수 파일로, (합법성을 조건으로) 크롤링해도 괜찮은 것과 로봇이 뒤져서는 안 되는 것을 명시합니다.\n",
    "\n",
    "파이썬에서는 웹 스크래핑과 관련하여 선택의 폭이 넓습니다. 다양한 사용자 스타일과 요구 사항을 다루는 매우 강력한 라이브러리가 5개 있습니다. **requests**, **lxml**, **beautifulsoup**, **selenium** 및 *scrapy**입니다.\n",
    "\n",
    "빠르고 간단한 웹 스크래핑의 경우 일반적인 조합은 웹 페이지의 HTML을 가져오는 것 외에는 거의 아무것도 하지 않는 **requests**와 페이지 구조를 탐색하고 실제로 관심 있는 것을 가져오는 데 도움이 되는 **beautifulsoup**입니다. HTML뿐만 아니라 자바스크립트를 사용하는 동적 웹 페이지의 경우 **selenium**이 필요합니다. 효율적인 방식으로 수천 개의 웹 페이지를 방문하려면 다른 도구와 함께 작동하고 여러 세션을 처리할 수 있으며 다른 모든 종류의 부가 기능을 갖춘 **scrapy**를 사용해 볼 수 있습니다. 이것은 실제로 \"웹 스크래핑 프레임워크\"입니다.\n",
    "\n",
    "실제로 코딩을 보는 것은 항상 도움이 되므로 지금 그렇게 할 것이지만 사용자 에이전트, 스크래핑 요청에 '정중하게' 대처하는 것, 캐싱 및 크롤링을 효율적으로 수행하는 것과 같은 많은 중요한 세부 정보를 건너뛸 것입니다.\n",
    "\n",
    "더 나은 예가 없으므로 [http://aeturrell.com/](http://aeturrell.com/)의 연구 페이지를 스크랩해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073d89e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://aeturrell.com/research\"\n",
    "page = requests.get(url)\n",
    "page.text[:300]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8e85230",
   "metadata": {},
   "source": [
    "자, 방금 무슨 일이 있었나요? requests에 웹 페이지의 HTML을 가져오도록 요청한 다음 찾은 텍스트의 처음 300자를 인쇄했습니다.\n",
    "\n",
    "이제 beautifulsoup를 사용하여 사람이 읽을 수 있는(또는 더 쉽게 읽을 수 있는) 것으로 구문 분석해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f96be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "print(soup.prettify()[60000:60500])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5748e928",
   "metadata": {},
   "source": [
    "이제 페이지 구조가 더 잘 보이고 'title' 및 'link'와 같은 *HTML 태그*도 일부 보입니다. 이제 데이터 추출 부분입니다. 텍스트의 모든 단락을 가져오고 싶다고 가정해 보겠습니다. beautifulsoup를 사용하여 HTML 구조를 훑어보고 단락 태그('p')가 있는 부분만 가져올 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82775de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 단락 가져오기\n",
    "all_paras = soup.find_all(\"p\")\n",
    "# 단락 중 하나만 표시\n",
    "all_paras[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2936677e",
   "metadata": {},
   "source": [
    "이 단락은 그다지 나쁘지 않지만 `.text` 메서드를 사용하여 HTML 태그를 완전히 제거하면 더 읽기 쉽게 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11321154",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_paras[1].text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d9d890e",
   "metadata": {},
   "source": [
    "이제 페이지의 대부분에 관심이 없고 프로젝트 이름만 가져오고 싶다고 가정해 보겠습니다. 이를 위해서는 관심 있는 요소의 태그 유형(이 경우 'div')과 클래스 유형(이 경우 \"project-name\")을 식별해야 합니다. 다음과 같이 수행합니다(그리고 그 과정에서 멋진 텍스트를 보여줌)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feac1fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = soup.find_all(\"div\", class_=\"project-content listing-pub-info\")\n",
    "projects = [x.text.strip() for x in projects]\n",
    "projects"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "461271a3",
   "metadata": {},
   "source": [
    "만세! 원하는 정보를 얻는 데 성공했습니다. 올바른 태그만 알면 되었습니다. 원하는 정보의 태그를 찾는 좋은 팁은 브라우저(예: Google Chrome)에서 해당 정보를 보고 관심 있는 부분을 마우스 오른쪽 버튼으로 클릭한 다음 '검사'를 누르는 것입니다. 이렇게 하면 클릭한 페이지 부분의 HTML 요소가 표시됩니다.\n",
    "\n",
    "이것으로 웹 스크래핑에 대한 매우 간략한 소개는 거의 끝났습니다. 한 가지만 더 살펴보겠습니다. 여러 페이지를 반복하는 방법입니다.\n",
    "\n",
    "\"www.codingforeconomists.com\"과 같은 루트 웹 페이지가 있고 \"www.codingforeconomists.com/page=1\", \"www.codingforeconomists.com/page=2\" 등과 같은 하위 페이지가 있다고 상상해 보십시오. 각 페이지를 스크랩하고 관련 데이터를 반환하는 함수에 전달할 HTML 문자열을 반복적으로 만들기만 하면 됩니다. 예를 들어 처음 50개 페이지에 대해 `scraper()`라는 함수를 사용하여 다음과 같이 실행할 수 있습니다.\n",
    "\n",
    "```\n",
    "start, stop = 0, 50\n",
    "root_url = \"www.codingforeconomists.com/page=\"\n",
    "info_on_pages = [scraper(root_url + str(i)) for i in range(start, stop)]\n",
    "```\n",
    "\n",
    "여기서 다룰 내용은 이것이 전부이지만 이 크고 복잡한 주제의 극히 일부만 *긁어모았을* 뿐이라는 점을 기억하십시오. 응용 프로그램에 대해 읽고 싶다면 의심할 여지 없이 세상을 가장 많이 바꾸었고 여러분 자신의 삶에도 수많은 방식으로 영향을 미쳤을 웹 스크래핑에 대한 논문인 Page, Brin, Motwani 및 Winograd의 [\"PageRank 인용 순위: 웹에 질서 부여하기\"](http://ilpubs.stanford.edu:8090/422/)를 추천하지 않을 수 없습니다. 웹 스크래핑에 대한 더 자세한 예는 realpython의 [튜토리얼](https://realpython.com/python-web-scraping-practical-introduction/)을 확인하십시오."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52ce838c",
   "metadata": {},
   "source": [
    "### 웹 스크래핑 테이블\n",
    "\n",
    "전체 웹 페이지를 스크랩하고 싶지 않고 페이지 내 *테이블*의 데이터만 원하는 경우가 많습니다. 다행히 **pandas** 패키지를 사용하여 개별 테이블을 쉽게 스크랩할 수 있는 방법이 있습니다.\n",
    "\n",
    "**pandas**를 사용하여 'https://simple.wikipedia.org/wiki/FIFA_World_Cup'의 첫 번째 테이블에서 데이터를 읽을 것입니다. 사용할 함수는 `read_html()`이며, URL을 전달하면 찾은 모든 테이블의 데이터 프레임 목록을 반환합니다. 테이블 목록을 필터링하려면 관심 있는 테이블에만 나타나는 텍스트와 함께 `match=` 키워드 인수를 사용합니다.\n",
    "\n",
    "아래 예는 이것이 어떻게 작동하는지 보여줍니다. 웹사이트를 보면 관심 있는 테이블(과거 월드컵 결과)에 '4위' 열이 있지만 페이지의 다른 테이블에는 없음을 알 수 있습니다. 따라서 다음을 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ada9ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = pd.read_html(\n",
    "    \"https://simple.wikipedia.org/wiki/FIFA_World_Cup\", match=\"Sweden\"\n",
    ")\n",
    "# 데이터 프레임 목록에서 첫 번째이자 유일한 항목 검색\n",
    "df = df_list[0]\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31e49317",
   "metadata": {},
   "source": [
    "이렇게 하면 테이블이 **pandas** 데이터 프레임에 깔끔하게 로드되어 추가 사용 준비가 완료됩니다."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "md:myst",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
